{"cells":[{"cell_type":"markdown","source":["NOTEBOOK CREATED BY SHREYAS KAWALE IN 'CLUS' DATABRICKS CLUSTER- CLUSTER HELPS IN RESOURCE MANAGEMENT AND COST OPTIMIZATION"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"478932fe-5436-443f-9493-c87e661c933b","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# File location and type\nfile_location = \"/FileStore/tables/car_sales.csv\"\nfile_type = \"csv\"\ndf_car= spark.read.csv(file_location, header=True, inferSchema=True)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"6482be4c-f067-47c9-b0ac-35c938b94601","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df_car.head(7)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"662d631d-4654-4c96-928d-59d44477515c","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[10]: [Row(Make='Toyota', Colour='White', Odometer (KM)=150043, Doors=4, Price='$4,000.00'),\n Row(Make='Honda', Colour='Red', Odometer (KM)=87899, Doors=4, Price='$5,000.00'),\n Row(Make='Toyota', Colour='Blue', Odometer (KM)=32549, Doors=3, Price='$7,000.00'),\n Row(Make='BMW', Colour='Black', Odometer (KM)=11179, Doors=5, Price='$22,000.00'),\n Row(Make='Nissan', Colour='White', Odometer (KM)=213095, Doors=4, Price='$3,500.00'),\n Row(Make='Toyota', Colour='Green', Odometer (KM)=99213, Doors=4, Price='$4,500.00'),\n Row(Make='Honda', Colour='Blue', Odometer (KM)=45698, Doors=4, Price='$7,500.00')]"]}],"execution_count":0},{"cell_type":"code","source":["#[Odometer(Km),Doors]----> new feature--->independent feature\n#SAMPLE PREDICTION WITH THESE TWO COLUMNS"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"b3555b48-9006-4059-affc-7a7c01672db0","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.ml.feature import VectorAssembler\nfeatureassembler=VectorAssembler(inputCols=[\"Odometer (KM)\",\"Doors\"],outputCol=\"Independent Features\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"e4f669f4-fdc3-484a-8ddd-6abf8c99a4db","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["output=featureassembler.transform(df_car)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"29e91d8b-bf28-4bdf-affa-115d437ef00f","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ffd1163d-601a-4873-811e-c793108e560e","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["output.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"d8005108-9d82-4aae-a14f-92ad1f0884a4","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+------+------+-------------+-----+----------+--------------------+\n|  Make|Colour|Odometer (KM)|Doors|     Price|Independent Features|\n+------+------+-------------+-----+----------+--------------------+\n|Toyota| White|       150043|    4| $4,000.00|      [150043.0,4.0]|\n| Honda|   Red|        87899|    4| $5,000.00|       [87899.0,4.0]|\n|Toyota|  Blue|        32549|    3| $7,000.00|       [32549.0,3.0]|\n|   BMW| Black|        11179|    5|$22,000.00|       [11179.0,5.0]|\n|Nissan| White|       213095|    4| $3,500.00|      [213095.0,4.0]|\n|Toyota| Green|        99213|    4| $4,500.00|       [99213.0,4.0]|\n| Honda|  Blue|        45698|    4| $7,500.00|       [45698.0,4.0]|\n| Honda|  Blue|        54738|    4| $7,000.00|       [54738.0,4.0]|\n|Toyota| White|        60000|    4| $6,250.00|       [60000.0,4.0]|\n|Nissan| White|        31600|    4| $9,700.00|       [31600.0,4.0]|\n+------+------+-------------+-----+----------+--------------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["finalized_data=output.select(\"Independent Features\",\"Price\")\nfinalized_data.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"0c3791d9-4280-46f4-b5e5-1317eb8b2954","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+--------------------+----------+\n|Independent Features|     Price|\n+--------------------+----------+\n|      [150043.0,4.0]| $4,000.00|\n|       [87899.0,4.0]| $5,000.00|\n|       [32549.0,3.0]| $7,000.00|\n|       [11179.0,5.0]|$22,000.00|\n|      [213095.0,4.0]| $3,500.00|\n|       [99213.0,4.0]| $4,500.00|\n|       [45698.0,4.0]| $7,500.00|\n|       [54738.0,4.0]| $7,000.00|\n|       [60000.0,4.0]| $6,250.00|\n|       [31600.0,4.0]| $9,700.00|\n+--------------------+----------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.ml.regression import LinearRegression\n##train test split\ntrain_data,test_data=finalized_data.randomSplit([0.75,0.25])\nregressor=LinearRegression(featuresCol='Independent Features', labelCol='Price')\nregressor=regressor.fit(train_data)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"2543fb78-f03a-45c6-a6b1-bdd38cdaf139","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mIllegalArgumentException\u001B[0m                  Traceback (most recent call last)\nFile \u001B[0;32m<command-617528596635789>:5\u001B[0m\n\u001B[1;32m      3\u001B[0m train_data,test_data\u001B[38;5;241m=\u001B[39mfinalized_data\u001B[38;5;241m.\u001B[39mrandomSplit([\u001B[38;5;241m0.75\u001B[39m,\u001B[38;5;241m0.25\u001B[39m])\n\u001B[1;32m      4\u001B[0m regressor\u001B[38;5;241m=\u001B[39mLinearRegression(featuresCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mIndependent Features\u001B[39m\u001B[38;5;124m'\u001B[39m, labelCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mPrice\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m----> 5\u001B[0m regressor\u001B[38;5;241m=\u001B[39mregressor\u001B[38;5;241m.\u001B[39mfit(train_data)\n\nFile \u001B[0;32m/databricks/python_shell/dbruntime/MLWorkloadsInstrumentation/_pyspark.py:30\u001B[0m, in \u001B[0;36m_create_patch_function.<locals>.patched_method\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     28\u001B[0m call_succeeded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 30\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43moriginal_method\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     31\u001B[0m     call_succeeded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m     32\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/ml/base.py:205\u001B[0m, in \u001B[0;36mEstimator.fit\u001B[0;34m(self, dataset, params)\u001B[0m\n\u001B[1;32m    203\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcopy(params)\u001B[38;5;241m.\u001B[39m_fit(dataset)\n\u001B[1;32m    204\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 205\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    206\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    207\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[1;32m    208\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    209\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbut got \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mtype\u001B[39m(params)\n\u001B[1;32m    210\u001B[0m     )\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/ml/wrapper.py:383\u001B[0m, in \u001B[0;36mJavaEstimator._fit\u001B[0;34m(self, dataset)\u001B[0m\n\u001B[1;32m    382\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_fit\u001B[39m(\u001B[38;5;28mself\u001B[39m, dataset: DataFrame) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m JM:\n\u001B[0;32m--> 383\u001B[0m     java_model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit_java\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    384\u001B[0m     model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_model(java_model)\n\u001B[1;32m    385\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_copyValues(model)\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/ml/wrapper.py:380\u001B[0m, in \u001B[0;36mJavaEstimator._fit_java\u001B[0;34m(self, dataset)\u001B[0m\n\u001B[1;32m    377\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_java_obj \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    379\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_transfer_params_to_java()\n\u001B[0;32m--> 380\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_java_obj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mIllegalArgumentException\u001B[0m: requirement failed: Column Price must be of type numeric but was actually of type string.","errorSummary":"<span class='ansi-red-fg'>IllegalArgumentException</span>: requirement failed: Column Price must be of type numeric but was actually of type string.","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n","\u001B[0;31mIllegalArgumentException\u001B[0m                  Traceback (most recent call last)\n","File \u001B[0;32m<command-617528596635789>:5\u001B[0m\n","\u001B[1;32m      3\u001B[0m train_data,test_data\u001B[38;5;241m=\u001B[39mfinalized_data\u001B[38;5;241m.\u001B[39mrandomSplit([\u001B[38;5;241m0.75\u001B[39m,\u001B[38;5;241m0.25\u001B[39m])\n","\u001B[1;32m      4\u001B[0m regressor\u001B[38;5;241m=\u001B[39mLinearRegression(featuresCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mIndependent Features\u001B[39m\u001B[38;5;124m'\u001B[39m, labelCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mPrice\u001B[39m\u001B[38;5;124m'\u001B[39m)\n","\u001B[0;32m----> 5\u001B[0m regressor\u001B[38;5;241m=\u001B[39mregressor\u001B[38;5;241m.\u001B[39mfit(train_data)\n","\n","File \u001B[0;32m/databricks/python_shell/dbruntime/MLWorkloadsInstrumentation/_pyspark.py:30\u001B[0m, in \u001B[0;36m_create_patch_function.<locals>.patched_method\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n","\u001B[1;32m     28\u001B[0m call_succeeded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n","\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n","\u001B[0;32m---> 30\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43moriginal_method\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n","\u001B[1;32m     31\u001B[0m     call_succeeded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n","\u001B[1;32m     32\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n","\n","File \u001B[0;32m/databricks/spark/python/pyspark/ml/base.py:205\u001B[0m, in \u001B[0;36mEstimator.fit\u001B[0;34m(self, dataset, params)\u001B[0m\n","\u001B[1;32m    203\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcopy(params)\u001B[38;5;241m.\u001B[39m_fit(dataset)\n","\u001B[1;32m    204\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n","\u001B[0;32m--> 205\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m)\u001B[49m\n","\u001B[1;32m    206\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n","\u001B[1;32m    207\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n","\u001B[1;32m    208\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n","\u001B[1;32m    209\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbut got \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mtype\u001B[39m(params)\n","\u001B[1;32m    210\u001B[0m     )\n","\n","File \u001B[0;32m/databricks/spark/python/pyspark/ml/wrapper.py:383\u001B[0m, in \u001B[0;36mJavaEstimator._fit\u001B[0;34m(self, dataset)\u001B[0m\n","\u001B[1;32m    382\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_fit\u001B[39m(\u001B[38;5;28mself\u001B[39m, dataset: DataFrame) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m JM:\n","\u001B[0;32m--> 383\u001B[0m     java_model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit_java\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m)\u001B[49m\n","\u001B[1;32m    384\u001B[0m     model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_model(java_model)\n","\u001B[1;32m    385\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_copyValues(model)\n","\n","File \u001B[0;32m/databricks/spark/python/pyspark/ml/wrapper.py:380\u001B[0m, in \u001B[0;36mJavaEstimator._fit_java\u001B[0;34m(self, dataset)\u001B[0m\n","\u001B[1;32m    377\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_java_obj \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n","\u001B[1;32m    379\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_transfer_params_to_java()\n","\u001B[0;32m--> 380\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_java_obj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[43m)\u001B[49m\n","\n","File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n","\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n","\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n","\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n","\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n","\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n","\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n","\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n","\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n","\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n","\n","File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n","\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n","\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n","\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n","\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n","\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n","\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n","\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n","\n","\u001B[0;31mIllegalArgumentException\u001B[0m: requirement failed: Column Price must be of type numeric but was actually of type string."]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import col, regexp_replace\nfrom pyspark.sql.types import IntegerType, FloatType\n\n# Assuming you have a DataFrame named \"df\" with a \"price\" column containing values like \"$4,000.00\"\n\n# Remove the currency symbol ($) and comma (,) from the \"price\" column\ndf_car = df_car.withColumn(\"price\", regexp_replace(col(\"price\"), \"[$,]\", \"\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"5792479b-a301-44d3-94fd-1ad8360d7e6c","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df_car.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"3440e1a4-16e8-412f-ac11-2a33188e3432","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+------+------+-------------+-----+--------+\n|  Make|Colour|Odometer (KM)|Doors|   price|\n+------+------+-------------+-----+--------+\n|Toyota| White|       150043|    4| 4000.00|\n| Honda|   Red|        87899|    4| 5000.00|\n|Toyota|  Blue|        32549|    3| 7000.00|\n|   BMW| Black|        11179|    5|22000.00|\n|Nissan| White|       213095|    4| 3500.00|\n|Toyota| Green|        99213|    4| 4500.00|\n| Honda|  Blue|        45698|    4| 7500.00|\n| Honda|  Blue|        54738|    4| 7000.00|\n|Toyota| White|        60000|    4| 6250.00|\n|Nissan| White|        31600|    4| 9700.00|\n+------+------+-------------+-----+--------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df_car = df_car.withColumn(\"Price\", col(\"Price\").cast(FloatType()))\n\n# Convert the \"price\" column from float to integer\ndf_car = df_car.withColumn(\"Price\", col(\"Price\").cast(IntegerType()))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"47127ef3-8894-43ea-ab7f-e472001495db","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df_car.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"36ba3cd0-e7cf-42e8-a2c2-aa462964440c","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- Make: string (nullable = true)\n |-- Colour: string (nullable = true)\n |-- Odometer (KM): integer (nullable = true)\n |-- Doors: integer (nullable = true)\n |-- Price: integer (nullable = true)\n\n"]}],"execution_count":0},{"cell_type":"code","source":["# CODINT THE SAME MACHINE LEARNING\nfrom pyspark.ml.feature import VectorAssembler\nfeatureassembler=VectorAssembler(inputCols=[\"Odometer (KM)\",\"Doors\"],outputCol=\"Independent Features\")\noutput=featureassembler.transform(df_car)\nfinalized_data=output.select(\"Independent Features\",\"Price\")\nfrom pyspark.ml.regression import LinearRegression\n##train test split\ntrain_data,test_data=finalized_data.randomSplit([0.75,0.25])\nregressor=LinearRegression(featuresCol='Independent Features', labelCol='Price')\nregressor=regressor.fit(train_data)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"3e7ed8c1-b01d-49f6-94b9-2d82275a09f0","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["pred_results=regressor.evaluate(test_data)\npred_results.predictions.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"9709109b-fa9d-422d-ab3d-fed91fc4866f","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+--------------------+-----+------------------+\n|Independent Features|Price|        prediction|\n+--------------------+-----+------------------+\n|       [32549.0,3.0]| 7000|-3871.531555745576|\n|       [45698.0,4.0]| 7500| 8001.782638669749|\n|       [60000.0,4.0]| 6250| 7364.240365169411|\n|       [99213.0,4.0]| 4500| 5616.236927134953|\n|      [213095.0,4.0]| 3500|  539.702932535205|\n+--------------------+-----+------------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["pred_results.meanAbsoluteError,pred_results.meanSquaredError"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"997785fc-4e91-4f52-8027-93a00eead853","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[53]: (3312.817710836897, 25938571.876111083)"]}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"PYSPARK ML PREDICTOR","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
